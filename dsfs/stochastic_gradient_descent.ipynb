{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapting Gradient Descent for Numpy\n",
    "Adapted from Joel Grus' Data Science from scratch with a numpy implementation by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcarlisle/anaconda/envs/dsfs/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some inputs for doing SGD to find a linear regression fit\n",
    "# some random observations from 0-20\n",
    "x_obs = np.random.uniform(0,20,10)\n",
    "\n",
    "# target variable, x +/- some small noise - so the result of SGD should be ~ 1\n",
    "y_obs = x_obs + np.random.normal(loc=0,scale=0.1,size=10)\n",
    "\n",
    "# squared error, linear regression\n",
    "def sq_error(x, y, beta):\n",
    "  return (y-(x * beta[0])) ** 2\n",
    "\n",
    "# gradient of squared error for linear regression\n",
    "def sq_error_grad(x, y, beta):\n",
    "  return [(y-(x * beta[0])) * -2]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Gradient Descent Code\n",
    "This code comes from Data Science from Scratch. Available [here](https://github.com/joelgrus/data-science-from-scratch/blob/master/code/gradient_descent.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = zip(x, y)\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running SGD for the pure python function from DSFS\n",
    "converges to something like 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9960496477378477]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize_stochastic(sq_error, sq_error_grad, x_obs, y_obs, [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    \"\"\"\n",
    "    Numpy version:\n",
    "    target_fn: scoring function to minimize sum of across observations\n",
    "    gradient_fn: the gradient of the target fn with respect to each of\n",
    "                 the input dimensions of x\n",
    "    x: training variable(s) as ndarray\n",
    "    y: target variable as array\n",
    "    theta_0: starting point\n",
    "    alpha_0: initial gradient step magnitude\n",
    "    \"\"\"\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "    data_idx = range(0, x.shape[0])\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = target_fn(x, y, theta).sum()\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for i in np.random.choice(data_idx, size=len(data_idx), replace=False):\n",
    "            #print i\n",
    "            gradient_i = gradient_fn(x[i,:], y[i], theta)\n",
    "            #print gradient_i\n",
    "            theta = theta - (alpha * gradient_i)\n",
    "\n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out the numpy version\n",
    "I made another dimension for X and a new y s.t. `y2_obs = x_obs + x2_obs + noise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_obs = np.random.uniform(-10,10,10)\n",
    "\n",
    "X = np.array([x_obs, x2_obs]).T\n",
    "\n",
    "y2_obs = X.sum(axis=1) + np.random.normal(loc=0,scale=0.1,size=10)\n",
    "\n",
    "# an error function defined for numpy ndarrays\n",
    "def multi_error(x, y, beta):\n",
    "    return (y - np.matmul(beta,x.T))\n",
    "\n",
    "# squared error\n",
    "def sq_error_multi(x, y, beta):\n",
    "    return np.square(multi_error(x, y, beta)) \n",
    "\n",
    "# the gradient for this numpy version of the squared error\n",
    "def sq_error_multi_grad(x, y, beta):\n",
    "    return -x * multi_error(x,y,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting new y vs the row-wise sum of X. Looks basically linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x116871a50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113eba250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x=X.sum(axis=1), y=y2_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run my numpy SGD with an initial guess of 5,5 for the coefficients on x, x2. Converges to something like 1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0046187 , 1.00395797])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_minimize_stochastic(sq_error_multi, sq_error_multi_grad, X, y2_obs, np.array([5,5]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsfs]",
   "language": "python",
   "name": "conda-env-dsfs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
