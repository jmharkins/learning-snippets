{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Deep Neural Network for MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\")#, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders for batch inputs/outputs of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, n_inputs], dtype=tf.float32, name='X')\n",
    "y = tf.placeholder(shape=None, dtype=tf.int64, name='X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the network**:\n",
    "This a NN with 2 hidden layers, both with ReLu activation, with 300 & 100 neurons, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of \n",
    "n_neur_w1 = 300 \n",
    "n_neur_w2 = 100 \n",
    "\n",
    "# layer 1\n",
    "stddev_1 = 2 / np.sqrt(n_inputs)\n",
    "init_1 = tf.truncated_normal((n_inputs, n_neur_w1), stddev=stddev_1)\n",
    "W_1 = tf.Variable(init_1, name='W1')\n",
    "b_1 = tf.Variable(tf.zeros([n_neur_w1]), name=\"bias_1\")\n",
    "z_1 = tf.matmul(X, W_1) + b_1\n",
    "activ_1 = tf.nn.relu(z_1)\n",
    "# layer 2\n",
    "stddev_2 = 2 / np.sqrt(n_neur_w1)\n",
    "init_2 = tf.truncated_normal((n_neur_w1, n_neur_w2), stddev=stddev_2)\n",
    "W_2 = tf.Variable(init_2, name='W2')\n",
    "b_2 = tf.Variable(tf.zeros([n_neur_w2]), name=\"bias_2\")\n",
    "z_2 = tf.matmul(activ_1, W_2) + b_2\n",
    "activ_2 = tf.nn.relu(z_2)\n",
    "# output\n",
    "stddev_out = 2 / np.sqrt(n_neur_w2)\n",
    "init_out = tf.truncated_normal((n_neur_w2, n_outputs), stddev=stddev_out)\n",
    "W_out = tf.Variable(init_out, name='W2')\n",
    "b_out = tf.Variable(tf.zeros([n_outputs]), name=\"bias_2\")\n",
    "logits = tf.matmul(activ_2, W_out) + b_out\n",
    "\n",
    "# loss\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimization steps\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train accuracy:', 0.86000001, 'Val accuracy:', 0.9174)\n",
      "(1, 'Train accuracy:', 0.83999997, 'Val accuracy:', 0.93699998)\n",
      "(2, 'Train accuracy:', 0.95999998, 'Val accuracy:', 0.94419998)\n",
      "(3, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.94940001)\n",
      "(4, 'Train accuracy:', 0.92000002, 'Val accuracy:', 0.9526)\n",
      "(5, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.95679998)\n",
      "(6, 'Train accuracy:', 1.0, 'Val accuracy:', 0.958)\n",
      "(7, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.9598)\n",
      "(8, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.96240002)\n",
      "(9, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.96439999)\n",
      "(10, 'Train accuracy:', 1.0, 'Val accuracy:', 0.96579999)\n",
      "(11, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.96719998)\n",
      "(12, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9666)\n",
      "(13, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9702)\n",
      "(14, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97140002)\n",
      "(15, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.97000003)\n",
      "(16, 'Train accuracy:', 0.95999998, 'Val accuracy:', 0.97140002)\n",
      "(17, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97079998)\n",
      "(18, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.97259998)\n",
      "(19, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.97180003)\n",
      "(20, 'Train accuracy:', 0.94, 'Val accuracy:', 0.97280002)\n",
      "(21, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.97320002)\n",
      "(22, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97399998)\n",
      "(23, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9756)\n",
      "(24, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97579998)\n",
      "(25, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97539997)\n",
      "(26, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9752)\n",
      "(27, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9752)\n",
      "(28, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97420001)\n",
      "(29, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97640002)\n",
      "(30, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97600001)\n",
      "(31, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97680002)\n",
      "(32, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97619998)\n",
      "(33, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9756)\n",
      "(34, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.97659999)\n",
      "(35, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97680002)\n",
      "(36, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97719997)\n",
      "(37, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.977)\n",
      "(38, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97820002)\n",
      "(39, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97799999)\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neural-nets]",
   "language": "python",
   "name": "conda-env-neural-nets-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
