{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST:\n",
    "This notebook constructs a 5-layer neural network to predict MNIST labels 0-4 based on exercise 8 in [Hands-on Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28038 2558 5139\n"
     ]
    }
   ],
   "source": [
    "print len(X_train1), len(X_valid1), len(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST\n",
    "n_outputs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, n_inputs], dtype=tf.float32, name='X')\n",
    "y = tf.placeholder(shape=None, dtype=tf.int64, name='X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Neural Network Structure:**\n",
    "\n",
    "This network has 5 layers with 100 neurons each. The layers are initialized using _He_ initialization and use an _ELU_ activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable hidden1/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-32-16a3f1521573>\", line 9, in <module>\n    name=\"hidden1\")\n  File \"/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-514c1c0315d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                            \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhe_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                            \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                            name=\"hidden1\")\n\u001b[0m\u001b[1;32m     10\u001b[0m hidden_2 = tf.layers.dense(inputs=hidden_1,\n\u001b[1;32m     11\u001b[0m                            \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_neur\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/layers/core.pyc\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 218\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \"\"\"\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/layers/core.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    121\u001b[0m                                   \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                                   \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                                   trainable=True)\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       self.bias = vs.get_variable('bias',\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36mvariable_getter\u001b[0;34m(getter, name, shape, dtype, initializer, regularizer, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m           variable_getter=functools.partial(getter, **kwargs))\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;31m# Build (if necessary) and call the layer, inside a variable scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36m_add_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, variable_getter)\u001b[0m\n\u001b[1;32m    226\u001b[0m                                \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                                \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                                trainable=trainable and self.trainable)\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;31m# TODO(sguada) fix name = variable.op.name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable hidden1/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-32-16a3f1521573>\", line 9, in <module>\n    name=\"hidden1\")\n  File \"/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/sarahcarlisle/anaconda/envs/neural-nets/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# number of neurons by layer:\n",
    "n_neur = 100\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "hidden_1 = tf.layers.dense(inputs=X,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden1\")\n",
    "hidden_2 = tf.layers.dense(inputs=hidden_1,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden2\")\n",
    "hidden_3 = tf.layers.dense(inputs=hidden_2,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden3\")\n",
    "hidden_4 = tf.layers.dense(inputs=hidden_3,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden4\")\n",
    "hidden_5 = tf.layers.dense(inputs=hidden_4,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden5\")\n",
    "logits = tf.layers.dense(inputs=hidden_5,\n",
    "                         units=n_outputs,\n",
    "                         kernel_initializer=he_init,\n",
    "                         name='logits')\n",
    "# define individual cross entropy and overall loss for a batch\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization:**\n",
    "\n",
    "Some hyper-parameters for optimizing the weights of this network:\n",
    "- learning rate (_eta_) = 0.01\n",
    "- optimizer = Adam optimization with default hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999)\n",
    "training_step = optimizer.minimize(loss, name='training_step')\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**   \n",
    "This training procedure implements early stopping to avoid overfitting. The stopping scheme works as follows: store the best loss value as a new one is encountered, if this value has not changed in 20 iterations, preserve the model that achieved this loss and stop training. I had initially set the batch size to 50 and got good results, which kind of thwarted the next stage of the exercise to implement some anti-overfitting strategies. So re-setting to 20 obtained the desired result of getting an overfit model to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "stopping_threshold = 20\n",
    "iter_since_best = 0\n",
    "best_loss_val = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = X_train1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train accuracy:', 1.0, 'Val accuracy:', 0.94370604)\n",
      "(1, 'Train accuracy:', 1.0, 'Val accuracy:', 0.96638)\n",
      "(2, 'Train accuracy:', 1.0, 'Val accuracy:', 0.18725567)\n",
      "(3, 'Train accuracy:', 1.0, 'Val accuracy:', 0.20914777)\n",
      "(4, 'Train accuracy:', 1.0, 'Val accuracy:', 0.22009382)\n",
      "(5, 'Train accuracy:', 1.0, 'Val accuracy:', 0.22009382)\n",
      "(6, 'Train accuracy:', 1.0, 'Val accuracy:', 0.20914777)\n",
      "(7, 'Train accuracy:', 1.0, 'Val accuracy:', 0.20914777)\n",
      "(8, 'Train accuracy:', 1.0, 'Val accuracy:', 0.1927287)\n",
      "(9, 'Train accuracy:', 1.0, 'Val accuracy:', 0.20914777)\n",
      "(10, 'Train accuracy:', 1.0, 'Val accuracy:', 0.19077404)\n",
      "(11, 'Train accuracy:', 1.0, 'Val accuracy:', 0.22009382)\n",
      "(12, 'Train accuracy:', 1.0, 'Val accuracy:', 0.19077404)\n",
      "(13, 'Train accuracy:', 1.0, 'Val accuracy:', 0.22009382)\n",
      "(14, 'Train accuracy:', 1.0, 'Val accuracy:', 0.22009382)\n",
      "(15, 'Train accuracy:', 1.0, 'Val accuracy:', 0.20914777)\n",
      "(16, 'Train accuracy:', 1.0, 'Val accuracy:', 0.22009382)\n",
      "(17, 'Train accuracy:', 1.0, 'Val accuracy:', 0.18725567)\n",
      "(18, 'Train accuracy:', 1.0, 'Val accuracy:', 0.22009382)\n",
      "(19, 'Train accuracy:', 1.0, 'Val accuracy:', 0.22009382)\n",
      "(20, 'Train accuracy:', 1.0, 'Val accuracy:', 0.1927287)\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 95.56%\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        rand_idx = np.random.permutation(np.arange(n_samples))\n",
    "        for iteration in range(n_samples // batch_size):\n",
    "            batch_idx = rand_idx[(iteration*batch_size):((iteration+1)*batch_size)]\n",
    "            X_batch = X_train1[batch_idx,:]\n",
    "            y_batch = y_train1[batch_idx]\n",
    "            sess.run(training_step, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val, )\n",
    "        if epoch % 10 == 0:\n",
    "            if loss_val < best_loss_val:\n",
    "                save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "                best_loss_val = loss_val\n",
    "                iter_since_best = 0\n",
    "        if iter_since_best >= stopping_threshold:\n",
    "            break\n",
    "        iter_since_best +=1\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Better performance, batch size 50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "stopping_threshold = 20\n",
    "iter_since_best = 0\n",
    "best_loss_val = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = X_train1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97458953)\n",
      "(1, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97615325)\n",
      "(2, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98436278)\n",
      "(3, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97928071)\n",
      "(4, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97810787)\n",
      "(5, 'Train accuracy:', 1.0, 'Val accuracy:', 0.86708367)\n",
      "(6, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97615325)\n",
      "(7, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97888976)\n",
      "(8, 'Train accuracy:', 1.0, 'Val accuracy:', 0.77677876)\n",
      "(9, 'Train accuracy:', 1.0, 'Val accuracy:', 0.77834243)\n",
      "(10, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97849882)\n",
      "(11, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98162627)\n",
      "(12, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97849882)\n",
      "(13, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98553556)\n",
      "(14, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98279905)\n",
      "(15, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98358095)\n",
      "(16, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97732604)\n",
      "(17, 'Train accuracy:', 1.0, 'Val accuracy:', 0.93158716)\n",
      "(18, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98397183)\n",
      "(19, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98358095)\n",
      "(20, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98084444)\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 97.26%\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        rand_idx = np.random.permutation(np.arange(n_samples))\n",
    "        for iteration in range(n_samples // batch_size):\n",
    "            batch_idx = rand_idx[(iteration*batch_size):((iteration+1)*batch_size)]\n",
    "            X_batch = X_train1[batch_idx,:]\n",
    "            y_batch = y_train1[batch_idx]\n",
    "            sess.run(training_step, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val, )\n",
    "        if epoch % 10 == 0:\n",
    "            if loss_val < best_loss_val:\n",
    "                save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "                best_loss_val = loss_val\n",
    "                iter_since_best = 0\n",
    "        if iter_since_best >= stopping_threshold:\n",
    "            break\n",
    "        iter_since_best +=1\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I also tried a smaller learning rate which seemed to really boost things in terms of performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding batch normalization\n",
    "\n",
    "Trying to use Batch Normalization to improve convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, n_inputs], dtype=tf.float32, name='X')\n",
    "y = tf.placeholder(shape=None, dtype=tf.int64, name='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = tf.layers.dense(inputs=X,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden1\")\n",
    "bn_h1 = tf.layers.batch_normalization(hidden_1,training=training,momentum=0.9)\n",
    "h1_act = tf.nn.elu(bn_h1)\n",
    "hidden_2 = tf.layers.dense(inputs=h1_act,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden2\")\n",
    "bn_h2 = tf.layers.batch_normalization(hidden_2,training=training,momentum=0.9)\n",
    "h2_act = tf.nn.elu(bn_h2)\n",
    "hidden_3 = tf.layers.dense(inputs=h2_act,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden3\")\n",
    "bn_h3 = tf.layers.batch_normalization(hidden_3,training=training,momentum=0.9)\n",
    "h3_act = tf.nn.elu(bn_h3)\n",
    "hidden_4 = tf.layers.dense(inputs=h3_act,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden4\")\n",
    "bn_h4 = tf.layers.batch_normalization(hidden_4,training=training,momentum=0.9)\n",
    "h4_act = tf.nn.elu(bn_h4)\n",
    "hidden_5 = tf.layers.dense(inputs=h4_act,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden5\")\n",
    "bn_h5 = tf.layers.batch_normalization(hidden_5,training=training,momentum=0.9)\n",
    "h5_act = tf.nn.elu(bn_h5)\n",
    "pre_logits = tf.layers.dense(inputs=h5_act,\n",
    "                             units=n_outputs,\n",
    "                             kernel_initializer=he_init,\n",
    "                             name='logits')\n",
    "logits = tf.layers.batch_normalization(pre_logits,training=training,momentum=0.9) \n",
    "# define individual cross entropy and overall loss for a batch\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999)\n",
    "training_step = optimizer.minimize(loss, name='training_step')\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_since_best = 0\n",
    "best_loss_val = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97849882)\n",
      "(1, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98162627)\n",
      "(2, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98436278)\n",
      "(3, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99061769)\n",
      "(4, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98514462)\n",
      "(5, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98709929)\n",
      "(6, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98201722)\n",
      "(7, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98788118)\n",
      "(8, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(9, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99022675)\n",
      "(10, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99100858)\n",
      "(11, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(12, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99061769)\n",
      "(13, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98788118)\n",
      "(14, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98788118)\n",
      "(15, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99022675)\n",
      "(16, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99179047)\n",
      "(17, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(18, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98905396)\n",
      "(19, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99257231)\n",
      "(20, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(21, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9933542)\n",
      "(22, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99139953)\n",
      "(23, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98944485)\n",
      "(24, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(25, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99413604)\n",
      "(26, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99530882)\n",
      "(27, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99061769)\n",
      "(28, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99257231)\n",
      "(29, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(30, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99257231)\n",
      "(31, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99491793)\n",
      "(32, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98709929)\n",
      "(33, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99218142)\n",
      "(34, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9933542)\n",
      "(35, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(36, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99218142)\n",
      "(37, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99218142)\n",
      "(38, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9933542)\n",
      "(39, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99257231)\n",
      "(40, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99218142)\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 99.40%\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        rand_idx = np.random.permutation(np.arange(n_samples))\n",
    "        for iteration in range(n_samples // batch_size):\n",
    "            batch_idx = rand_idx[(iteration*batch_size):((iteration+1)*batch_size)]\n",
    "            X_batch = X_train1[batch_idx,:]\n",
    "            y_batch = y_train1[batch_idx]\n",
    "            sess.run([training_step, extra_update_ops],\n",
    "                     feed_dict={training:True, X: X_batch, y: y_batch}\n",
    "                    )\n",
    "        loss_val, acc_val = sess.run([loss, accuracy],\n",
    "                                     feed_dict={X: X_valid1, y: y_valid1})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val, )\n",
    "        if epoch % 10 == 0:\n",
    "            if loss_val < best_loss_val:\n",
    "                save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "                best_loss_val = loss_val\n",
    "                iter_since_best = 0\n",
    "        if iter_since_best >= stopping_threshold:\n",
    "            break\n",
    "        iter_since_best +=1\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like performance got better but convergence did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, n_inputs], dtype=tf.float32, name='X')\n",
    "y = tf.placeholder(shape=None, dtype=tf.int64, name='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = tf.layers.dense(inputs=X,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden1\")\n",
    "bn_h1 = tf.layers.batch_normalization(hidden_1,training=training,momentum=0.9)\n",
    "h1_act = tf.nn.elu(bn_h1)\n",
    "h1_drop = tf.layers.dropout(h1_act, dropout_rate, training=training)\n",
    "hidden_2 = tf.layers.dense(inputs=h1_drop,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden2\")\n",
    "bn_h2 = tf.layers.batch_normalization(hidden_2,training=training,momentum=0.9)\n",
    "h2_act = tf.nn.elu(bn_h2)\n",
    "h2_drop = tf.layers.dropout(h2_act, dropout_rate, training=training)\n",
    "hidden_3 = tf.layers.dense(inputs=h2_drop,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden3\")\n",
    "bn_h3 = tf.layers.batch_normalization(hidden_3,training=training,momentum=0.9)\n",
    "h3_act = tf.nn.elu(bn_h3)\n",
    "h3_drop = tf.layers.dropout(h3_act, dropout_rate, training=training)\n",
    "hidden_4 = tf.layers.dense(inputs=h3_drop,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden4\")\n",
    "bn_h4 = tf.layers.batch_normalization(hidden_4,training=training,momentum=0.9)\n",
    "h4_act = tf.nn.elu(bn_h4)\n",
    "h4_drop = tf.layers.dropout(h4_act, dropout_rate, training=training)\n",
    "hidden_5 = tf.layers.dense(inputs=h4_drop,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=he_init,\n",
    "                           name=\"hidden5\")\n",
    "bn_h5 = tf.layers.batch_normalization(hidden_5,training=training,momentum=0.9)\n",
    "h5_act = tf.nn.elu(bn_h5)\n",
    "h5_drop = tf.layers.dropout(h5_act, dropout_rate, training=training)\n",
    "pre_logits = tf.layers.dense(inputs=h5_drop,\n",
    "                             units=n_outputs,\n",
    "                             kernel_initializer=he_init,\n",
    "                             name='logits')\n",
    "logits = tf.layers.batch_normalization(pre_logits,training=training,momentum=0.9) \n",
    "# define individual cross entropy and overall loss for a batch\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999)\n",
    "training_step = optimizer.minimize(loss, name='training_step')\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_since_best = 0\n",
    "best_loss_val = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train accuracy:', 1.0, 'Val accuracy:', 0.96833462)\n",
      "(1, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97498047)\n",
      "(2, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97732604)\n",
      "(3, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97928071)\n",
      "(4, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98397183)\n",
      "(5, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98592651)\n",
      "(6, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98436278)\n",
      "(7, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98514462)\n",
      "(8, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98553556)\n",
      "(9, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98709929)\n",
      "(10, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98553556)\n",
      "(11, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98827207)\n",
      "(12, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98944485)\n",
      "(13, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98905396)\n",
      "(14, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98944485)\n",
      "(15, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99100858)\n",
      "(16, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99139953)\n",
      "(17, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98827207)\n",
      "(18, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98670834)\n",
      "(19, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(20, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98827207)\n",
      "(21, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98905396)\n",
      "(22, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98866302)\n",
      "(23, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(24, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(25, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98944485)\n",
      "(26, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99061769)\n",
      "(27, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(28, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98905396)\n",
      "(29, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98905396)\n",
      "(30, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99022675)\n",
      "(31, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98944485)\n",
      "(32, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99061769)\n",
      "(33, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(34, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99100858)\n",
      "(35, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99022675)\n",
      "(36, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99257231)\n",
      "(37, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(38, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99100858)\n",
      "(39, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99061769)\n",
      "(40, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99022675)\n",
      "(41, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98866302)\n",
      "(42, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99022675)\n",
      "(43, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(44, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99179047)\n",
      "(45, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99139953)\n",
      "(46, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99139953)\n",
      "(47, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(48, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99100858)\n",
      "(49, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(50, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99022675)\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 99.36%\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        rand_idx = np.random.permutation(np.arange(n_samples))\n",
    "        for iteration in range(n_samples // batch_size):\n",
    "            batch_idx = rand_idx[(iteration*batch_size):((iteration+1)*batch_size)]\n",
    "            X_batch = X_train1[batch_idx,:]\n",
    "            y_batch = y_train1[batch_idx]\n",
    "            sess.run([training_step, extra_update_ops],\n",
    "                     feed_dict={training:True, X: X_batch, y: y_batch}\n",
    "                    )\n",
    "        loss_val, acc_val = sess.run([loss, accuracy],\n",
    "                                     feed_dict={X: X_valid1, y: y_valid1})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val, )\n",
    "        if epoch % 10 == 0:\n",
    "            if loss_val < best_loss_val:\n",
    "                save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "                best_loss_val = loss_val\n",
    "                iter_since_best = 0\n",
    "        if iter_since_best >= stopping_threshold:\n",
    "            break\n",
    "        iter_since_best +=1\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout looks like it didn't do too much improvement over the last model with Batch Norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neural-nets]",
   "language": "python",
   "name": "conda-env-neural-nets-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
