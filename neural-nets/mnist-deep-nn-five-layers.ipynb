{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28038 2558 5139\n"
     ]
    }
   ],
   "source": [
    "print len(X_train1), len(X_valid1), len(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST\n",
    "n_outputs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, n_inputs], dtype=tf.float32, name='X')\n",
    "y = tf.placeholder(shape=None, dtype=tf.int64, name='X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Neural Network Structure:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of neurons by layer:\n",
    "n_neur = 100\n",
    "\n",
    "hidden_1 = tf.layers.dense(inputs=X,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden1\")\n",
    "hidden_2 = tf.layers.dense(inputs=hidden_1,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden2\")\n",
    "hidden_3 = tf.layers.dense(inputs=hidden_2,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden3\")\n",
    "hidden_4 = tf.layers.dense(inputs=hidden_3,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden4\")\n",
    "hidden_5 = tf.layers.dense(inputs=hidden_4,\n",
    "                           units=n_neur,\n",
    "                           kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                           activation=tf.nn.elu,\n",
    "                           name=\"hidden5\")\n",
    "logits = tf.layers.dense(inputs=hidden_5,\n",
    "                         units=n_outputs,\n",
    "                         kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                         name='logits')\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999)\n",
    "training_step = optimizer.minimize(loss, name='training_step')\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "stopping_threshold = 20\n",
    "iter_since_best = 0\n",
    "best_loss_val = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = X_train1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train accuracy:', 1.0, 'Val accuracy:', 0.97849882)\n",
      "(1, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.98592651)\n",
      "(2, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98827207)\n",
      "(3, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98827207)\n",
      "(4, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98788118)\n",
      "(5, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98866302)\n",
      "(6, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98827207)\n",
      "(7, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99179047)\n",
      "(8, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98397183)\n",
      "(9, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98944485)\n",
      "(10, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98358095)\n",
      "(11, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(12, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99179047)\n",
      "(13, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98631746)\n",
      "(14, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98749024)\n",
      "(15, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99257231)\n",
      "(16, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98944485)\n",
      "(17, 'Train accuracy:', 0.98000002, 'Val accuracy:', 0.98905396)\n",
      "(18, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99100858)\n",
      "(19, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98709929)\n",
      "(20, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(21, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99218142)\n",
      "(22, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99257231)\n",
      "(23, 'Train accuracy:', 1.0, 'Val accuracy:', 0.98084444)\n",
      "(24, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99179047)\n",
      "(25, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99179047)\n",
      "(26, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9898358)\n",
      "(27, 'Train accuracy:', 1.0, 'Val accuracy:', 0.9933542)\n",
      "(28, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99413604)\n",
      "(29, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99452698)\n",
      "(30, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99139953)\n",
      "(31, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99100858)\n",
      "(32, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99139953)\n",
      "(33, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99452698)\n",
      "(34, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99139953)\n",
      "(35, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(36, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99374509)\n",
      "(37, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(38, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(39, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(40, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(41, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(42, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(43, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(44, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(45, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(46, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(47, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(48, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(49, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "(50, 'Train accuracy:', 1.0, 'Val accuracy:', 0.99296325)\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 99.10%\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        rand_idx = np.random.permutation(np.arange(n_samples))\n",
    "        for iteration in range(n_samples // batch_size):\n",
    "            batch_idx = rand_idx[(iteration*batch_size):((iteration+1)*batch_size)]\n",
    "            X_batch = X_train1[batch_idx,:]\n",
    "            y_batch = y_train1[batch_idx]\n",
    "            sess.run(training_step, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid1,\n",
    "                                           y: y_valid1})\n",
    "        loss_val = loss.eval(feed_dict={X: X_valid1,\n",
    "                                           y: y_valid1})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val, )\n",
    "        if epoch % 10 == 0:\n",
    "            if loss_val < best_loss_val:\n",
    "                save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "                best_loss_val = loss_val\n",
    "                iter_since_best = 0\n",
    "        if iter_since_best >= stopping_threshold:\n",
    "            break\n",
    "        iter_since_best +=1\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neural-nets]",
   "language": "python",
   "name": "conda-env-neural-nets-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
